{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCpBgczAbJtL",
        "outputId": "9ae29f7b-efc6-40bd-9213-3227673b92b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "0tutYnUQR8bx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxIUOCYfRveb"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import gc\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration Parameters"
      ],
      "metadata": {
        "id": "FbvrXJ0LSPyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration parameters\n",
        "TARGET = \"y\"\n",
        "\n",
        "# XGBoost parameters\n",
        "XGB_PARAMS = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'eta': 0.03,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 2000,\n",
        "    'random_state': SEED,\n",
        "    'tree_method': 'hist',\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "# LightGBM parameters\n",
        "LGB_PARAMS = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 64,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'n_estimators': 2000,\n",
        "    'seed': SEED,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# CatBoost parameters\n",
        "CAT_PARAMS = {\n",
        "    'loss_function': 'Logloss',\n",
        "    'eval_metric': 'AUC',\n",
        "    'depth': 8,\n",
        "    'learning_rate': 0.03,\n",
        "    'l2_leaf_reg': 3,\n",
        "    'random_seed': SEED,\n",
        "    'iterations': 2000,\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "# Create directories for saving models and predictions\n",
        "SAVE_DIR = Path(\"models\")\n",
        "SAVE_DIR.mkdir(exist_ok=True)\n",
        "SAVE_PRED = Path(\"oof_preds\")\n",
        "SAVE_PRED.mkdir(exist_ok=True)\n",
        "OUT_DIR = Path(\"submissions\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n"
      ],
      "metadata": {
        "id": "e0H3NdO6SD-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "QdRUVWyBS6CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load parquet data file\"\"\"\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "def get_folds(df: pd.DataFrame, n_splits: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"Create stratified folds for cross-validation\"\"\"\n",
        "    df = df.copy()\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    df[\"fold\"] = -1\n",
        "\n",
        "    for i, (_, val_idx) in enumerate(skf.split(df, df[TARGET])):\n",
        "        df.loc[val_idx, \"fold\"] = i\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "wB-CD3JSSZno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l536pKW3Ti5w",
        "outputId": "36c9f947-fd71-4e65-bac4-5b3c38c4e2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting config\n",
            "  Downloading config-0.5.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: config\n",
            "Successfully installed config-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metric"
      ],
      "metadata": {
        "id": "V_NsrdZPTH_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib, gc\n",
        "from pathlib import Path\n",
        "import xgboost as xgb # Import xgboost\n",
        "from xgboost import XGBClassifier # Keep import for reference, but won't use for training\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# from config import XGB_PARAMS, LGB_PARAMS, CAT_PARAMS, TARGET, SEED # Removed the import from config\n",
        "\n",
        "SAVE_DIR = Path(\"models\")\n",
        "SAVE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def train_xgb(X_train, y_train, X_val, y_val, name):\n",
        "    \"\"\"Train XGBoost model using native API with early stopping\"\"\"\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "    eval_set = [(dtrain, 'train'), (dval, 'eval')]\n",
        "\n",
        "    # Use xgb.train with early stopping parameters\n",
        "    model = xgb.train(\n",
        "        XGB_PARAMS,\n",
        "        dtrain,\n",
        "        num_boost_round=XGB_PARAMS['n_estimators'], # Use n_estimators from params\n",
        "        evals=eval_set,\n",
        "        early_stopping_rounds=200, # early_stopping_rounds in xgb.train\n",
        "        verbose_eval=False # Set to True for detailed progress\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model.save_model(SAVE_DIR / f\"{name}.xgb\") # Save in native xgb format\n",
        "\n",
        "    # Print validation score\n",
        "    val_pred = model.predict(dval) # Use model.predict for DMatrix\n",
        "    val_score = roc_auc_score(y_val, val_pred)\n",
        "    print(f\"XGBoost {name} - Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_lgb(X_train, y_train, X_val, y_val, name):\n",
        "    \"\"\"Train LightGBM model with early stopping\"\"\"\n",
        "    model = LGBMClassifier(**LGB_PARAMS)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        # Using early_stopping_rounds directly in fit for LGBM\n",
        "        callbacks=[model.callback_early_stopping(stopping_rounds=200, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, SAVE_DIR / f\"{name}.joblib\")\n",
        "\n",
        "    # Print validation score\n",
        "    val_pred = model.predict_proba(X_val)[:, 1]\n",
        "    val_score = roc_auc_score(y_val, val_pred)\n",
        "    print(f\"LightGBM {name} - Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_cat(X_train, y_train, X_val, y_val, name):\n",
        "    \"\"\"Train CatBoost model with early stopping\"\"\"\n",
        "    model = CatBoostClassifier(**CAT_PARAMS)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        early_stopping_rounds=200, # CatBoost still uses early_stopping_rounds\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, SAVE_DIR / f\"{name}.joblib\")\n",
        "\n",
        "    # Print validation score\n",
        "    val_pred = model.predict_proba(X_val)[:, 1]\n",
        "    val_score = roc_auc_score(y_val, val_pred)\n",
        "    print(f\"CatBoost {name} - Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_lr_meta(meta_X, meta_y):\n",
        "    \"\"\"Train meta-learner (Logistic Regression)\"\"\"\n",
        "    lr = make_pipeline(\n",
        "        StandardScaler(with_mean=False),\n",
        "        LogisticRegression(max_iter=1000, n_jobs=-1, random_state=SEED)\n",
        "    )\n",
        "    lr.fit(meta_X, meta_y)\n",
        "    joblib.dump(lr, SAVE_DIR / \"meta_lr.joblib\")\n",
        "\n",
        "    # Print meta-learner score\n",
        "    meta_pred = lr.predict_proba(meta_X)[:, 1]\n",
        "    meta_score = roc_auc_score(meta_y, meta_pred)\n",
        "    print(f\"Meta-learner - Training AUC: {meta_score:.4f}\")\n",
        "\n",
        "    return lr"
      ],
      "metadata": {
        "id": "HV0mmDnASslI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5CGFXshZjJQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "DPemRxLXUPTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgb(X_train, y_train, X_val, y_val, name):\n",
        "    \"\"\"Train XGBoost model with early stopping\"\"\"\n",
        "    model = XGBClassifier(**XGB_PARAMS)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        # early_stopping_rounds=200, # Removed due to TypeError\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, SAVE_DIR / f\"{name}.joblib\")\n",
        "\n",
        "    # Print validation score\n",
        "    val_pred = model.predict_proba(X_val)[:, 1]\n",
        "    val_score = roc_auc_score(y_val, val_pred)\n",
        "    print(f\"XGBoost {name} - Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_lgb(X_train, y_train, X_val, y_val, name):\n",
        "    \"\"\"Train LightGBM model with early stopping\"\"\"\n",
        "    model = LGBMClassifier(**LGB_PARAMS)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=[\n",
        "            model.callback_early_stopping(stopping_rounds=200, verbose=False)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, SAVE_DIR / f\"{name}.joblib\")\n",
        "\n",
        "    # Print validation score\n",
        "    val_pred = model.predict_proba(X_val)[:, 1]\n",
        "    val_score = roc_auc_score(y_val, val_pred)\n",
        "    print(f\"LightGBM {name} - Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_cat(X_train, y_train, X_val, y_val, name):\n",
        "    \"\"\"Train CatBoost model with early stopping\"\"\"\n",
        "    model = CatBoostClassifier(**CAT_PARAMS)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        early_stopping_rounds=200,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, SAVE_DIR / f\"{name}.joblib\")\n",
        "\n",
        "    # Print validation score\n",
        "    val_pred = model.predict_proba(X_val)[:, 1]\n",
        "    val_score = roc_auc_score(y_val, val_pred)\n",
        "    print(f\"CatBoost {name} - Validation AUC: {val_score:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_lr_meta(meta_X, meta_y):\n",
        "    \"\"\"Train meta-learner (Logistic Regression)\"\"\"\n",
        "    lr = make_pipeline(\n",
        "        StandardScaler(with_mean=False),\n",
        "        LogisticRegression(max_iter=1000, n_jobs=-1, random_state=SEED)\n",
        "    )\n",
        "    lr.fit(meta_X, meta_y)\n",
        "    joblib.dump(lr, SAVE_DIR / \"meta_lr.joblib\")\n",
        "\n",
        "    # Print meta-learner score\n",
        "    meta_pred = lr.predict_proba(meta_X)[:, 1]\n",
        "    meta_score = roc_auc_score(meta_y, meta_pred)\n",
        "    print(f\"Meta-learner - Training AUC: {meta_score:.4f}\")\n",
        "\n",
        "    return lr"
      ],
      "metadata": {
        "id": "nV4T3thFT9Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare Data"
      ],
      "metadata": {
        "id": "kP3UM9rJaw2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your combined dataset\n",
        "# Replace 'df_train.parquet' with your actual file path\n",
        "df_train = load_data(\"/content/drive/MyDrive/Amex/combined/train_sample_25p.parquet\")\n",
        "\n",
        "# Create folds for cross-validation\n",
        "df_train = get_folds(df_train)\n",
        "\n",
        "# Identify feature columns (exclude id columns, target, and fold)\n",
        "feature_cols = [col for col in df_train.columns\n",
        "                if col not in ['fold', TARGET, 'id1', 'id2', 'id3', 'id4', 'id5']]\n",
        "\n",
        "print(f\"Dataset shape: {df_train.shape}\")\n",
        "print(f\"Number of features: {len(feature_cols)}\")\n",
        "print(f\"Target distribution: {df_train[TARGET].value_counts()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0saC5FOas9A",
        "outputId": "f29da7a0-6fde-495f-e976-90f4aa21e151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (220329, 392)\n",
            "Number of features: 385\n",
            "Target distribution: y\n",
            "0    183278\n",
            "1     37051\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validation Training Loop"
      ],
      "metadata": {
        "id": "IrH1-rribkyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize out-of-fold predictions\n",
        "oof_predictions = {\n",
        "    'xgb': np.zeros(len(df_train)),\n",
        "    'lgb': np.zeros(len(df_train)),\n",
        "    'cat': np.zeros(len(df_train))\n",
        "}\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold in range(5):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training Fold {fold + 1}/5\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Split data\n",
        "    train_idx = df_train[df_train.fold != fold].index\n",
        "    val_idx = df_train[df_train.fold == fold].index\n",
        "\n",
        "    X_train = df_train.loc[train_idx, feature_cols]\n",
        "    y_train = df_train.loc[train_idx, TARGET]\n",
        "    X_val = df_train.loc[val_idx, feature_cols]\n",
        "    y_val = df_train.loc[val_idx, TARGET]\n",
        "\n",
        "    # Convert target to integer type\n",
        "    y_train = y_train.astype(int)\n",
        "    y_val = y_val.astype(int)\n",
        "\n",
        "    # Identify and drop object type columns\n",
        "    object_cols = X_train.select_dtypes(include='object').columns\n",
        "    if len(object_cols) > 0:\n",
        "        print(f\"Dropping object columns: {list(object_cols)}\")\n",
        "        X_train = X_train.drop(columns=object_cols)\n",
        "        X_val = X_val.drop(columns=object_cols)\n",
        "        # Update feature_cols for subsequent folds if needed, although dropping here is sufficient for this fold\n",
        "\n",
        "    print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}\")\n",
        "\n",
        "    # Train models\n",
        "    xgb_model = train_xgb(X_train, y_train, X_val, y_val, f\"xgb_fold_{fold}\")\n",
        "    lgb_model = train_lgb(X_train, y_train, X_val, y_val, f\"lgb_fold_{fold}\")\n",
        "    cat_model = train_cat(X_train, y_train, X_val, y_val, f\"cat_fold_{fold}\")\n",
        "\n",
        "    # Store OOF predictions\n",
        "    oof_predictions['xgb'][val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n",
        "    oof_predictions['lgb'][val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n",
        "    oof_predictions['cat'][val_idx] = cat_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "\n",
        "    # Clean up memory\n",
        "    del xgb_model, lgb_model, cat_model\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "mjzHathabihP",
        "outputId": "23fea1b0-9562-466e-c0dc-95c551692bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training Fold 1/5\n",
            "==================================================\n",
            "Dropping object columns: ['f42', 'f48', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f349', 'f354', 'id6', 'id7', 'f368', 'f369', 'f370', 'f371', 'f372', 'id9', 'f377', 'id10', 'id11', 'f378', 'f374', 'id8', 'id12', 'id13']\n",
            "Train size: 176263, Validation size: 44066\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-3564903017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Train models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"xgb_fold_{fold}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mlgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"lgb_fold_{fold}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mcat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"cat_fold_{fold}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-2979547656.py\u001b[0m in \u001b[0;36mtrain_xgb\u001b[0;34m(X_train, y_train, X_val, y_val, name)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Train XGBoost model with early stopping\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mXGB_PARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1597\u001b[0m             )\n\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1600\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta learning"
      ],
      "metadata": {
        "id": "6dvrbN0tc86i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save OOF predictions\n",
        "for model_name, predictions in oof_predictions.items():\n",
        "    np.save(SAVE_PRED / f\"oof_{model_name}.npy\", predictions)\n",
        "\n",
        "# Calculate individual model scores\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Individual Model Performance (AUC)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for model_name, predictions in oof_predictions.items():\n",
        "    score = roc_auc_score(df_train[TARGET], predictions)\n",
        "    print(f\"{model_name.upper()}: {score:.4f}\")\n",
        "\n",
        "# Train meta-learner\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"Training Meta-Learner\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "meta_features = np.column_stack([\n",
        "    oof_predictions['xgb'],\n",
        "    oof_predictions['lgb'],\n",
        "    oof_predictions['cat']\n",
        "])\n",
        "\n",
        "meta_model = train_lr_meta(meta_features, df_train[TARGET].values)\n"
      ],
      "metadata": {
        "id": "jhs1G60Lc8lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Function"
      ],
      "metadata": {
        "id": "CWnYSFk9ddKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_base_model(model_name: str, test_features: pd.DataFrame, n_folds: int = 5):\n",
        "    \"\"\"Generate predictions from base model ensemble\"\"\"\n",
        "    predictions = np.zeros(len(test_features))\n",
        "\n",
        "    for fold in range(n_folds):\n",
        "        model = joblib.load(SAVE_DIR / f\"{model_name}_fold_{fold}.joblib\")\n",
        "        predictions += model.predict_proba(test_features)[:, 1] / n_folds\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def rank_blend(prediction_dict: dict, weights: dict):\n",
        "    \"\"\"Rank-based blending of predictions\"\"\"\n",
        "    # Convert predictions to ranks\n",
        "    ranks = {}\n",
        "    for model_name, preds in prediction_dict.items():\n",
        "        ranks[model_name] = preds.argsort().argsort()\n",
        "\n",
        "    # Weighted combination of ranks\n",
        "    blended_ranks = np.zeros_like(next(iter(ranks.values())), dtype=float)\n",
        "    for model_name, model_ranks in ranks.items():\n",
        "        blended_ranks += weights[model_name] * model_ranks\n",
        "\n",
        "    # Normalize to [0, 1] and invert (higher rank = higher probability)\n",
        "    blended_ranks = blended_ranks / blended_ranks.max()\n",
        "    return 1 - blended_ranks\n",
        "\n",
        "def weighted_average(prediction_dict: dict, weights: dict):\n",
        "    \"\"\"Weighted average of predictions\"\"\"\n",
        "    result = np.zeros_like(next(iter(prediction_dict.values())))\n",
        "    for model_name, preds in prediction_dict.items():\n",
        "        result += weights[model_name] * preds\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "N237ocFldR7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Load Test Data and Generate Predictions"
      ],
      "metadata": {
        "id": "_-UcoiHldp0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "# Replace 'df_test.parquet' with your actual test file path\n",
        "df_test = load_data(\"df_test.parquet\")\n",
        "\n",
        "# Generate base model predictions\n",
        "print(\"Generating base model predictions...\")\n",
        "test_predictions = {\n",
        "    'xgb': predict_base_model('xgb', df_test[feature_cols]),\n",
        "    'lgb': predict_base_model('lgb', df_test[feature_cols]),\n",
        "    'cat': predict_base_model('cat', df_test[feature_cols])\n",
        "}\n",
        "\n",
        "print(\"✅ Base model predictions generated\")\n",
        "\n",
        "# Generate ensemble predictions\n",
        "print(\"\\nGenerating ensemble predictions...\")\n",
        "\n",
        "# 1. Weighted soft voting\n",
        "soft_voting_weights = {'xgb': 0.4, 'lgb': 0.4, 'cat': 0.2}\n",
        "soft_voting_pred = weighted_average(test_predictions, soft_voting_weights)\n",
        "\n",
        "# 2. Rank-based blending\n",
        "rank_weights = {'xgb': 0.35, 'lgb': 0.35, 'cat': 0.30}\n",
        "rank_blend_pred = rank_blend(test_predictions, rank_weights)\n",
        "\n",
        "# 3. Stacking with meta-learner\n",
        "meta_model = joblib.load(SAVE_DIR / \"meta_lr.joblib\")\n",
        "meta_input = np.column_stack([\n",
        "    test_predictions['xgb'],\n",
        "    test_predictions['lgb'],\n",
        "    test_predictions['cat']\n",
        "])\n",
        "stacking_pred = meta_model.predict_proba(meta_input)[:, 1]\n",
        "\n",
        "# 4. Final ensemble (average of all methods)\n",
        "final_prediction = (soft_voting_pred + rank_blend_pred + stacking_pred) / 3\n",
        "\n"
      ],
      "metadata": {
        "id": "NFwWXOX1dpdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Submission File"
      ],
      "metadata": {
        "id": "RyDdIEQteE37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create submission dataframe\n",
        "submission = pd.DataFrame({\n",
        "    'id1': df_test['id1'],\n",
        "    'id2': df_test['id2'],\n",
        "    'id3': df_test['id3'],\n",
        "    'id5': df_test['id5'],\n",
        "    'pred': final_prediction.clip(0, 1)  # Ensure predictions are in [0, 1]\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission_path = OUT_DIR / \"ensemble_submission.csv\"\n",
        "submission.to_csv(submission_path, sep='\\t', index=False)\n",
        "\n",
        "print(f\"✅ Submission file saved: {submission_path}\")\n",
        "print(f\"Submission shape: {submission.shape}\")\n",
        "print(f\"Prediction statistics:\")\n",
        "print(f\"  Min: {final_prediction.min():.4f}\")\n",
        "print(f\"  Max: {final_prediction.max():.4f}\")\n",
        "print(f\"  Mean: {final_prediction.mean():.4f}\")\n",
        "print(f\"  Std: {final_prediction.std():.4f}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(submission.head())\n"
      ],
      "metadata": {
        "id": "aYwJn0iMeK4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze OOF predictions\n",
        "print(\"Out-of-Fold Predictions Analysis\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Calculate correlation between models\n",
        "oof_df = pd.DataFrame({\n",
        "    'xgb': oof_predictions['xgb'],\n",
        "    'lgb': oof_predictions['lgb'],\n",
        "    'cat': oof_predictions['cat'],\n",
        "    'target': df_train[TARGET]\n",
        "})\n",
        "\n",
        "print(\"Correlation matrix:\")\n",
        "print(oof_df.corr().round(3))\n",
        "\n",
        "# Create simple ensemble from OOF predictions\n",
        "oof_ensemble = (oof_predictions['xgb'] + oof_predictions['lgb'] + oof_predictions['cat']) / 3\n",
        "ensemble_score = roc_auc_score(df_train[TARGET], oof_ensemble)\n",
        "print(f\"\\nSimple ensemble OOF AUC: {ensemble_score:.4f}\")\n",
        "\n",
        "# Feature importance analysis (using last fold models as example)\n",
        "print(\"\\nFeature Importance Analysis\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Load last fold models for feature importance\n",
        "xgb_last = joblib.load(SAVE_DIR / \"xgb_fold_4.joblib\")\n",
        "lgb_last = joblib.load(SAVE_DIR / \"lgb_fold_4.joblib\")\n",
        "\n",
        "# Get feature importance\n",
        "xgb_importance = xgb_last.feature_importances_\n",
        "lgb_importance = lgb_last.feature_importances_\n",
        "\n",
        "# Create feature importance dataframe\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'xgb_importance': xgb_importance,\n",
        "    'lgb_importance': lgb_importance\n",
        "})\n",
        "\n",
        "# Average importance and sort\n",
        "importance_df['avg_importance'] = (importance_df['xgb_importance'] + importance_df['lgb_importance']) / 2\n",
        "importance_df = importance_df.sort_values('avg_importance', ascending=False)\n",
        "\n",
        "print(\"Top 10 most important features:\")\n",
        "print(importance_df.head(10)[['feature', 'avg_importance']])\n"
      ],
      "metadata": {
        "id": "BdBKqzCteXs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aefe375",
        "outputId": "30cbb3b0-cc87-4a01-efb7-16ca9dcc62ff"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    }
  ]
}